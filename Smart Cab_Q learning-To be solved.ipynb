{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#for text processing\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Origing, Destination, and Time of Pickup from the sms data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  As NLTK/Spacy depend on parts of speech tagging to identify location; the location names which don’t start with capital letter or which contains both hindi and english letters in it are not identified. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Example - (Dwarka/Hauz Khaz are Hindi/Urdu words) . These type of words are not trained under the Spacy NLP Model.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deal with this situation I tried to train a Blank Spacy NER Recognizer (CustomSpacy.ipynb) that will recognize the word like Dwarka,Hauz-Khaz as LOCATION.\n",
    "<a href=\"./CustomSpacy.ipynb\" >You can find the .ipynb file here </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Importing Custom Spacy Model as nlp2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_dir=Path(\".\\Models\")\n",
    "nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('dwarka sector 23', 'LOCATION'), ('airport leaving', 'LOCATION'), ('7 PM', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "docx = nlp2('I want to go to dwarka sector 23 from airport leaving at 7 PM')\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in docx.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pickup_drop(text):\n",
    "    \n",
    "    #Write your code here\n",
    "    #Extracting Entities LOCATION & TIME and doing little bit of manual cleaning\n",
    "    x=[]\n",
    "    orign,destination,time_of_pickup='','',''\n",
    "    time_of_pickup=''\n",
    "    #3text=text.lower()\n",
    "    text=text.replace(\"\\n\", \" \")\n",
    "    docx = nlp2(text)\n",
    "    for text1 in docx.ents:\n",
    "        \n",
    "        if text1.label_=='LOCATION':\n",
    "            text1 = text1.text\n",
    "            if 'airport' in text1:\n",
    "                text_len = len(text1)\n",
    "                if((text_len)>7):\n",
    "                    text1 = text1[0:7]\n",
    "            \n",
    "            if (re.search('from\\s'+text1+'',text) or re.search(''+text1+'\\sto',text)):\n",
    "                origin=text1\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            if (re.search('to\\s'+text1+'',text) or re.search('for\\s'+text1+'',text)):\n",
    "                destination=text1\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            #x.append(text)\n",
    "        else:\n",
    "            #print(time_of_pickup)\n",
    "            time_of_pickup = text1\n",
    "    return [origin, destination, time_of_pickup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hauz khaas', 'dwarka sector 23', 1 PM]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_pickup_drop('Kindly book a cab for me at 1 PM from hauz khaas to dwarka sector 23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the Q-Learning Process\n",
    "Breaking it down into steps, we get\n",
    "\n",
    "Initialize the Q-table by all zeros.\n",
    "\n",
    "Start exploring actions: \n",
    "\n",
    "For each state, select any one among all possible actions for the current state (S).\n",
    "\n",
    "Travel to the next state (S') as a result of that action (a).\n",
    "\n",
    "For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "\n",
    "Update Q-table values using the equation.\n",
    "\n",
    "Set the next state as the current state.\n",
    "\n",
    "If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting learned values\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called ϵ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Q_table\n",
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "frames = [] # for animation\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "##Write your code here\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "         # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "np.save(\"./q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 103343\n",
      "State: 214\n",
      "Action: 1\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-adcf58a03b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-150-adcf58a03b7e>\u001b[0m in \u001b[0;36mprint_frames\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained q_table for evaluation\n",
    "\n",
    "q_table = np.load(\"./q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_dict(city_df):\n",
    "    loc_dict = {}\n",
    "    \n",
    "    ## Create dictionary example, loc_dict['dwarka sector 23] = 0  \n",
    "    for  index,row in city_df.iterrows():\n",
    "        \n",
    "        loc_dict[row['location']] =  row['mapping']\n",
    "        \n",
    "    return loc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pick_up_drop_correction(pick_up, drop, line_num):\n",
    "    #write your code here\n",
    "        org_df = pd.read_csv(\"./org_df.csv\")\n",
    "        original_origin = org_df.iloc[line_num]['origin']\n",
    "        original_destination = org_df.iloc[line_num]['dest']\n",
    "#         print('original_origin :',original_origin)\n",
    "#         print('pick_up :',pick_up)\n",
    "#         print('Original Destination :',original_destination)\n",
    "#         print('drop :',drop)\n",
    "        #print('------------            -------------------')\n",
    "        if original_origin == pick_up and original_destination == drop:\n",
    "            return True\n",
    "        else:\n",
    "            print('original_origin :',original_origin)\n",
    "            print('pick_up :',pick_up)\n",
    "            print('Original Destination :',original_destination)\n",
    "            print('drop :',drop)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 episodes:\n",
      "Average timesteps per episode: 13.76\n",
      "Average penalties per episode: 0.045\n",
      "Total number of wrong predictions 0\n",
      "Total Reward is 6835\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "# 1) We need to take text drom \"sms.txt\" and fetch pickup and drop from it.\n",
    "# 2) Generate the random state from an enviroment and change the pick up and drop as the fetched one\n",
    "# 3) Evaluate you q_table performance on all the texts given in sms.txt.\n",
    "# 4) Have a check if the fetched pickup, drop is not matching with original pickup, drop using orig.csv\n",
    "# 5) If fetched pickup or/and drop does not match with the original, add penality and reward -10\n",
    "# 6) Calculate the Total reward, penalities, Wrong pickup/drop predicted and Average time steps per episode.\n",
    "\n",
    "total_epochs, total_penalties, total_reward, wrong_predictions = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "count = 0\n",
    "time_list = []\n",
    "f = open(\"./sms.txt\", \"r\")\n",
    "num_of_lines = 1000\n",
    "city = pd.read_csv(\"./city.csv\")\n",
    "frames = [] # for animation\n",
    "loc_dict = create_loc_dict(city)\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    lines=fetch_pickup_drop(line)\n",
    "    pick_up=lines[0]\n",
    "    drop=lines[1]\n",
    "    decision=check_pick_up_drop_correction(pick_up, drop, line_num)\n",
    "    \n",
    "    if not decision:\n",
    "        print(decision)\n",
    "        total_penalties +=1\n",
    "        reward = -10\n",
    "        total_reward += reward\n",
    "        wrong_predictions += 1\n",
    "    pickUp_idx = loc_dict[pick_up]\n",
    "    drop_idx = loc_dict[drop]\n",
    "\n",
    "    act_state = env.reset()\n",
    "    #print('Actual State --',act_state)\n",
    "    taxi_row,taxi_col,pick_up,drop = env.decode(act_state)\n",
    "    state = env.encode(taxi_row,taxi_col,int(pickUp_idx),int(drop_idx))\n",
    "    #print('New State Generated -- ',state)\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            #print('action--',action)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            #print('Reward -',reward)\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            \n",
    "            total_reward += reward\n",
    "            # Put each rendered frame into dict for animation\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    #wrong_predictions += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "    line_num +=1\n",
    "\n",
    "\n",
    "\"\"\" Printing Summary \"\"\"\n",
    "\n",
    "print(f\"Results after {num_of_lines} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_of_lines}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_of_lines}\")\n",
    "print(f\"Total number of wrong predictions\", wrong_predictions)\n",
    "print(\"Total Reward is\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

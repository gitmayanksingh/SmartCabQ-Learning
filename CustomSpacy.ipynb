{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Named Entity Recognizer (NER)\n",
    "##### Adding An Additional Entity (NER)\n",
    "+ Load the model\n",
    "    + spacy.load('en')\n",
    "     - Disable existing pipe line (nlp.disable_pipes)\n",
    "    + spacy.blank('en')\n",
    "     - Added Entity Recognizer to Pipeline\n",
    "+ Add a Label eg(ner.add_label(LABEL) & (nlp.begin_training())\n",
    "+ Shuffle and loop over the examples\n",
    "     - update the model (nlp.update)\n",
    "+ Save the trained model (nlp.to_disk)\n",
    "+ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "        (\"Please book a cab from airport to hauz khaas at 3 PM\", {\"entities\": [(23,30, \"LOCATION\"),(34, 44, 'LOCATION')]}),\n",
    "        (\"Kindly book a cab for me at 1 PM from hauz khaas to dwarka sector 23\", {\"entities\": [(38, 48, \"LOCATION\"),(52, 68, \"LOCATION\")]}),\n",
    "(\"Please book a cab from dwarka sector 23 to dwarka sector 21 at 3 PM\", {\"entities\": [(23,39, \"LOCATION\"),(43, 59, \"LOCATION\")]}), (\"I want to go to dwarka sector 23 from dwarka sector 21 leaving at 10 AM\",\n",
    "  {\"entities\": [(16,32, \"LOCATION\"),(38,54, 'LOCATION'),(66,71, 'TIME')]}),\n",
    "\n",
    " (\"Kindly book a cab for me at 12 AM from dwarka sector 21 to airport\",\n",
    "  {\"entities\": [(28,33, \"LOCATION\"),(39,55, 'LOCATION'),(59,66, 'TIME')]}),\n",
    "\n",
    "(\"hauz khaas to airport at 6 PM\",\n",
    "  {\"entities\": [(0,11, \"LOCATION\"),(14,21, 'LOCATION'),(25,30, 'TIME')]})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 67.61615478992462}\n",
      "Losses {'ner': 65.27188408374786}\n",
      "Losses {'ner': 60.89177322387695}\n",
      "Losses {'ner': 54.057390093803406}\n",
      "Losses {'ner': 47.42455333471298}\n",
      "Losses {'ner': 38.15659460425377}\n",
      "Losses {'ner': 31.4411893337965}\n",
      "Losses {'ner': 25.253413321916014}\n",
      "Losses {'ner': 24.792237991001457}\n",
      "Losses {'ner': 27.08376437611878}\n",
      "Losses {'ner': 28.38225531601347}\n",
      "Losses {'ner': 26.933002838632092}\n",
      "Losses {'ner': 24.60131986020133}\n",
      "Losses {'ner': 25.887421667575836}\n",
      "Losses {'ner': 21.633119106292725}\n",
      "Losses {'ner': 23.578279107809067}\n",
      "Losses {'ner': 24.97397291660309}\n",
      "Losses {'ner': 24.09725451655686}\n",
      "Losses {'ner': 21.32455402240157}\n",
      "Losses {'ner': 25.54311622492969}\n",
      "Losses {'ner': 23.96941846422851}\n",
      "Losses {'ner': 19.532400651834905}\n",
      "Losses {'ner': 21.45470502972603}\n",
      "Losses {'ner': 23.07109503215179}\n",
      "Losses {'ner': 24.241117200930603}\n",
      "Losses {'ner': 26.65587829548167}\n",
      "Losses {'ner': 29.74948314303765}\n",
      "Losses {'ner': 26.921938360785134}\n",
      "Losses {'ner': 25.628797798352025}\n",
      "Losses {'ner': 21.872189007059205}\n",
      "Losses {'ner': 25.27866521436954}\n",
      "Losses {'ner': 24.10691391211003}\n",
      "Losses {'ner': 24.23985683702631}\n",
      "Losses {'ner': 17.070182954426855}\n",
      "Losses {'ner': 15.708755657076836}\n",
      "Losses {'ner': 19.43078448390588}\n",
      "Losses {'ner': 16.28865377337206}\n",
      "Losses {'ner': 13.636300779879093}\n",
      "Losses {'ner': 11.424156009768922}\n",
      "Losses {'ner': 9.125548157065168}\n",
      "Losses {'ner': 11.452294011546655}\n",
      "Losses {'ner': 7.604165039792861}\n",
      "Losses {'ner': 8.162315852287065}\n",
      "Losses {'ner': 13.367730065021533}\n",
      "Losses {'ner': 7.307642220699279}\n",
      "Losses {'ner': 7.652459046883337}\n",
      "Losses {'ner': 9.415509381025018}\n",
      "Losses {'ner': 4.995505401773698}\n",
      "Losses {'ner': 8.431647313458349}\n",
      "Losses {'ner': 4.678673701178923}\n",
      "Losses {'ner': 4.342177115151812}\n",
      "Losses {'ner': 5.030158892075359}\n",
      "Losses {'ner': 7.478597198498979}\n",
      "Losses {'ner': 1.9435659525154616}\n",
      "Losses {'ner': 6.256908151198786}\n",
      "Losses {'ner': 7.218314549872895}\n",
      "Losses {'ner': 2.9419094734209743}\n",
      "Losses {'ner': 2.6763344843416235}\n",
      "Losses {'ner': 3.4931491036082205}\n",
      "Losses {'ner': 2.0064121725946196}\n",
      "Losses {'ner': 2.040345339566112}\n",
      "Losses {'ner': 2.2909608547550686}\n",
      "Losses {'ner': 1.4810688117770354}\n",
      "Losses {'ner': 3.017775838770323}\n",
      "Losses {'ner': 1.3000709227595433}\n",
      "Losses {'ner': 0.696734190053319}\n",
      "Losses {'ner': 4.993236763706894}\n",
      "Losses {'ner': 0.6636261451755141}\n",
      "Losses {'ner': 0.9663331489641788}\n",
      "Losses {'ner': 0.40994992017297704}\n",
      "Losses {'ner': 1.3700528001789791}\n",
      "Losses {'ner': 0.7227937411095927}\n",
      "Losses {'ner': 1.3666553776617074}\n",
      "Losses {'ner': 0.34515875029858506}\n",
      "Losses {'ner': 0.24043644837609593}\n",
      "Losses {'ner': 1.069351356346142}\n",
      "Losses {'ner': 0.7352775419054554}\n",
      "Losses {'ner': 2.493450889686986}\n",
      "Losses {'ner': 3.1588610564982127}\n",
      "Losses {'ner': 0.4661725353427109}\n",
      "Losses {'ner': 0.19504441763522812}\n",
      "Losses {'ner': 0.4472176556704177}\n",
      "Losses {'ner': 0.4665566890131022}\n",
      "Losses {'ner': 0.9214642689655546}\n",
      "Losses {'ner': 0.013259362588986983}\n",
      "Losses {'ner': 0.23454131446029758}\n",
      "Losses {'ner': 0.009231671520465118}\n",
      "Losses {'ner': 0.028188673628222086}\n",
      "Losses {'ner': 0.12194236493312528}\n",
      "Losses {'ner': 0.09517964292960576}\n",
      "Losses {'ner': 0.13340372111921509}\n",
      "Losses {'ner': 0.6016268758613376}\n",
      "Losses {'ner': 0.020435075775674185}\n",
      "Losses {'ner': 0.0039968925979028125}\n",
      "Losses {'ner': 0.2836933715832483}\n",
      "Losses {'ner': 0.003613994593891492}\n",
      "Losses {'ner': 0.0009421497841034107}\n",
      "Losses {'ner': 0.002462103869979498}\n",
      "Losses {'ner': 1.064066759219669}\n",
      "Losses {'ner': 0.03274078030478661}\n",
      "Entities [('airport', 'LOCATION'), ('hauz khaas', 'LOCATION')]\n",
      "Tokens [('Please', '', 2), ('book', '', 2), ('a', '', 2), ('cab', '', 2), ('from', '', 2), ('airport', 'LOCATION', 3), ('to', '', 2), ('hauz', 'LOCATION', 3), ('khaas', 'LOCATION', 1), ('at', '', 2), ('3', '', 2), ('PM', '', 2)]\n",
      "Entities [('12 AM', 'LOCATION'), ('dwarka sector 21', 'LOCATION'), ('airport', 'TIME')]\n",
      "Tokens [('Kindly', '', 2), ('book', '', 2), ('a', '', 2), ('cab', '', 2), ('for', '', 2), ('me', '', 2), ('at', '', 2), ('12', 'LOCATION', 3), ('AM', 'LOCATION', 1), ('from', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('21', 'LOCATION', 1), ('to', '', 2), ('airport', 'TIME', 3)]\n",
      "Entities [('hauz khaas', 'LOCATION'), ('airport', 'LOCATION')]\n",
      "Tokens [('hauz', 'LOCATION', 3), ('khaas', 'LOCATION', 1), ('to', '', 2), ('airport', 'LOCATION', 3), ('at', '', 2), ('6', '', 2), ('PM', '', 2)]\n",
      "Entities [('dwarka sector 23', 'LOCATION'), ('dwarka sector 21', 'LOCATION'), ('10 AM', 'TIME')]\n",
      "Tokens [('I', '', 2), ('want', '', 2), ('to', '', 2), ('go', '', 2), ('to', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('23', 'LOCATION', 1), ('from', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('21', 'LOCATION', 1), ('leaving', '', 2), ('at', '', 2), ('10', 'TIME', 3), ('AM', 'TIME', 1)]\n",
      "Entities [('hauz khaas', 'LOCATION'), ('dwarka sector 23', 'LOCATION')]\n",
      "Tokens [('Kindly', '', 2), ('book', '', 2), ('a', '', 2), ('cab', '', 2), ('for', '', 2), ('me', '', 2), ('at', '', 2), ('1', '', 2), ('PM', '', 2), ('from', '', 2), ('hauz', 'LOCATION', 3), ('khaas', 'LOCATION', 1), ('to', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('23', 'LOCATION', 1)]\n",
      "Entities [('dwarka sector 23', 'LOCATION'), ('dwarka sector 21', 'LOCATION')]\n",
      "Tokens [('Please', '', 2), ('book', '', 2), ('a', '', 2), ('cab', '', 2), ('from', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('23', 'LOCATION', 1), ('to', '', 2), ('dwarka', 'LOCATION', 3), ('sector', 'LOCATION', 1), ('21', 'LOCATION', 1), ('at', '', 2), ('3', '', 2), ('PM', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading the Trained Spacy NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_dir=Path(\"C:\\\\Users\\\\mayank singh\\\\Downloads\\\\NIT Warangal _file\\\\NLP\\\\MidTermProject2\\\\Models\")\n",
    "nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Testing on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('hauz khaas', 'LOCATION'), ('airport', 'LOCATION'), ('9 AM', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "docx = nlp2('hauz khaas to airport at 9 AM')\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in docx.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('dwarka sector 23', 'LOCATION'), ('airport leaving', 'LOCATION'), ('7 PM', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "docx = nlp2('I want to go to dwarka sector 23 from airport leaving at 7 PM')\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in docx.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('dwarka sector 21', 'LOCATION'), ('dwarka sector 23', 'LOCATION'), ('11 PM', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "docx = nlp2('I want to go to dwarka sector 21 from dwarka sector 23 leaving at 11 PM')\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in docx.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
